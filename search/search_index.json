{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Document-to-Podcast Blueprint","text":"<p>Blueprints empower developers to easily integrate AI capabilities into their projects using open-source models and tools.</p> <p>These docs are your companion to mastering the Document-to-Podcast Blueprint\u2014a local-first approach for transforming documents into engaging podcasts.</p>"},{"location":"#built-with","title":"Built with","text":"<ul> <li>Python 3.10+</li> <li>Llama-cpp (text-to-text, i.e script generation)</li> <li>Streamlit (UI demo)</li> </ul>"},{"location":"#get-started-quickly","title":"\ud83d\ude80 Get Started Quickly","text":""},{"location":"#start-building-your-own-document-to-podcast-pipeline-in-minutes","title":"Start building your own Document-to-Podcast pipeline in minutes:","text":"<ul> <li>Getting Started: Quick setup and installation instructions.</li> </ul>"},{"location":"#understand-the-system","title":"\ud83d\udd0d Understand the System","text":""},{"location":"#dive-deeper-into-how-the-blueprint-works","title":"Dive deeper into how the Blueprint works:","text":"<ul> <li>Step-by-Step Guide: A detailed breakdown of the system\u2019s design and workflow.</li> <li>API Reference: Explore the technical details of the core modules.</li> </ul>"},{"location":"#make-it-yours","title":"\ud83c\udfa8 Make It Yours","text":""},{"location":"#customize-the-blueprint-to-fit-your-needs","title":"Customize the Blueprint to fit your needs:","text":"<ul> <li>Customization Guide: Tailor prompts, voices, and settings to create unique podcasts.</li> </ul>"},{"location":"#join-the-community","title":"\ud83c\udf1f Join the Community","text":""},{"location":"#help-shape-the-future-of-blueprints","title":"Help shape the future of Blueprints:","text":"<ul> <li>Future Features &amp; Contributions: Learn about exciting upcoming features and how to contribute to the project.</li> </ul> <p>Have more questions? Reach out to us on Discord and we'll see how we can help:</p> <p></p>"},{"location":"#why-blueprints","title":"Why Blueprints?","text":"<p>Blueprints are more than starter code\u2014they\u2019re your gateway to building AI-powered solutions with confidence. With step-by-step guidance, modular design, and open-source tools, we make AI accessible for developers of all skill levels.</p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#document_to_podcast.preprocessing.data_cleaners","title":"<code>document_to_podcast.preprocessing.data_cleaners</code>","text":""},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_html","title":"<code>clean_html(text)</code>","text":"<p>Clean HTML text.</p> This function removes <ul> <li>scripts</li> <li>styles</li> <li>links</li> <li>meta tags</li> </ul> <p>In addition, it calls clean_with_regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_html(\"&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello,  world!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\")\n\"Hello, world!\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The HTML text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_html(text: str) -&gt; str:\n    \"\"\"Clean HTML text.\n\n    This function removes:\n        - scripts\n        - styles\n        - links\n        - meta tags\n\n    In addition, it calls [clean_with_regex][document_to_podcast.preprocessing.data_cleaners.clean_with_regex].\n\n    Examples:\n        &gt;&gt;&gt; clean_html(\"&lt;html&gt;&lt;body&gt;&lt;p&gt;Hello,  world!  &lt;/p&gt;&lt;/body&gt;&lt;/html&gt;\"\")\n        \"Hello, world!\"\n\n    Args:\n        text (str): The HTML text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    soup = BeautifulSoup(text, \"html.parser\")\n    for tag in soup([\"script\", \"style\", \"link\", \"meta\"]):\n        tag.decompose()\n    text = soup.get_text()\n    return clean_with_regex(text)\n</code></pre>"},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_markdown","title":"<code>clean_markdown(text)</code>","text":"<p>Clean Markdown text.</p> This function removes <ul> <li>markdown images</li> </ul> <p>In addition, it calls clean_with_regex.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_markdown('# Title   with image ![alt text](image.jpg \"Image Title\")')\n\"Title with image\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The Markdown text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_markdown(text: str) -&gt; str:\n    \"\"\"Clean Markdown text.\n\n    This function removes:\n        - markdown images\n\n    In addition, it calls [clean_with_regex][document_to_podcast.preprocessing.data_cleaners.clean_with_regex].\n\n    Examples:\n        &gt;&gt;&gt; clean_markdown('# Title   with image ![alt text](image.jpg \"Image Title\")')\n        \"Title with image\"\n\n    Args:\n        text (str): The Markdown text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    text = re.sub(r'!\\[.*?\\]\\(.*?(\".*?\")?\\)', \"\", text)\n\n    return clean_with_regex(text)\n</code></pre>"},{"location":"api/#document_to_podcast.preprocessing.data_cleaners.clean_with_regex","title":"<code>clean_with_regex(text)</code>","text":"<p>Clean text using regular expressions.</p> This function removes <ul> <li>URLs</li> <li>emails</li> <li>special characters</li> <li>extra spaces</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; clean_with_regex(\"\u00a0Hello,   world! http://example.com\")\n\"Hello, world!\"\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to clean.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The cleaned text.</p> Source code in <code>src/document_to_podcast/preprocessing/data_cleaners.py</code> <pre><code>def clean_with_regex(text: str) -&gt; str:\n    \"\"\"\n    Clean text using regular expressions.\n\n    This function removes:\n        - URLs\n        - emails\n        - special characters\n        - extra spaces\n\n    Examples:\n        &gt;&gt;&gt; clean_with_regex(\"\\xa0Hello,   world! http://example.com\")\n        \"Hello, world!\"\n\n    Args:\n        text (str): The text to clean.\n\n    Returns:\n        str: The cleaned text.\n    \"\"\"\n    text = re.sub(\n        r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n        \"\",\n        text,\n    )\n    text = re.sub(r\"[\\w\\.-]+@[\\w\\.-]+\\.[\\w]+\", \"\", text)\n    text = re.sub(r'[^a-zA-Z0-9\\s.,!?;:\"\\']', \"\", text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders","title":"<code>document_to_podcast.inference.model_loaders</code>","text":""},{"location":"api/#document_to_podcast.inference.model_loaders.TTSModel","title":"<code>TTSModel</code>  <code>dataclass</code>","text":"<p>The purpose of this class is to provide a unified interface for all the TTS models supported. Specifically, different TTS model families have different peculiarities, for example, the bark models need a BarkProcessor, the parler models need their own tokenizer, etc. This wrapper takes care of this complexity so that the user doesn't have to deal with it.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>InterfaceGGUF</code> <p>A TTS model that has a .generate() method or similar that takes text as input, and returns an audio in the form of a numpy array.</p> required <code>model_id</code> <code>str</code> <p>The model's identifier string.</p> required <code>sample_rate</code> <code>int</code> <p>The sample rate of the audio, required for properly saving the audio to a file.</p> required <code>custom_args</code> <code>dict</code> <p>Any model-specific arguments that a TTS model might require, e.g. tokenizer.</p> required Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>@dataclass\nclass TTSModel:\n    \"\"\"\n    The purpose of this class is to provide a unified interface for all the TTS models supported.\n    Specifically, different TTS model families have different peculiarities, for example, the bark models need a\n    BarkProcessor, the parler models need their own tokenizer, etc. This wrapper takes care of this complexity so that\n    the user doesn't have to deal with it.\n\n    Args:\n        model (InterfaceGGUF): A TTS model that has a .generate() method or similar\n            that takes text as input, and returns an audio in the form of a numpy array.\n        model_id (str): The model's identifier string.\n        sample_rate (int): The sample rate of the audio, required for properly saving the audio to a file.\n        custom_args (dict): Any model-specific arguments that a TTS model might require, e.g. tokenizer.\n    \"\"\"\n\n    model: InterfaceGGUF\n    model_id: str\n    sample_rate: int\n    custom_args: field(default_factory=dict)\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders._load_oute_tts","title":"<code>_load_oute_tts(model_id, **kwargs)</code>","text":"<p>Loads the given model_id using the OuteTTS interface. For more info: https://github.com/edwko/OuteTTS</p> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{org}/{repo}/{filename}</code>.</p> required <code>language</code> <code>str</code> <p>Supported languages in 0.2-500M: en, zh, ja, ko.</p> required <p>Returns:</p> Name Type Description <code>TTSModel</code> <code>TTSModel</code> <p>The loaded model using the TTSModel wrapper.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def _load_oute_tts(model_id: str, **kwargs) -&gt; TTSModel:\n    \"\"\"\n    Loads the given model_id using the OuteTTS interface. For more info: https://github.com/edwko/OuteTTS\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{org}/{repo}/{filename}`.\n        language (str): Supported languages in 0.2-500M: en, zh, ja, ko.\n\n    Returns:\n        TTSModel: The loaded model using the TTSModel wrapper.\n    \"\"\"\n    model_version = model_id.split(\"-\")[1]\n\n    org, repo, filename = model_id.split(\"/\")\n    local_path = hf_hub_download(repo_id=f\"{org}/{repo}\", filename=filename)\n    model_config = GGUFModelConfig_v1(\n        model_path=local_path,\n        language=kwargs.pop(\"language\", \"en\"),\n        n_gpu_layers=-1 if torch.cuda.is_available else 0,\n        additional_model_config={\"verbose\": False},\n    )\n    model = InterfaceGGUF(model_version=model_version, cfg=model_config)\n\n    return TTSModel(\n        model=model, model_id=model_id, sample_rate=model.audio_codec.sr, custom_args={}\n    )\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders.load_llama_cpp_model","title":"<code>load_llama_cpp_model(model_id)</code>","text":"<p>Loads the given model_id using Llama.from_pretrained.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; model = load_llama_cpp_model(\"bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model_id</code> <code>str</code> <p>The model id to load. Format is expected to be <code>{org}/{repo}/{filename}</code>.</p> required <p>Returns:</p> Name Type Description <code>Llama</code> <code>Llama</code> <p>The loaded model.</p> Source code in <code>src/document_to_podcast/inference/model_loaders.py</code> <pre><code>def load_llama_cpp_model(model_id: str) -&gt; Llama:\n    \"\"\"\n    Loads the given model_id using Llama.from_pretrained.\n\n    Examples:\n        &gt;&gt;&gt; model = load_llama_cpp_model(\"bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf\")\n\n    Args:\n        model_id (str): The model id to load.\n            Format is expected to be `{org}/{repo}/{filename}`.\n\n    Returns:\n        Llama: The loaded model.\n    \"\"\"\n    org, repo, filename = model_id.split(\"/\")\n    model = Llama.from_pretrained(\n        repo_id=f\"{org}/{repo}\",\n        filename=filename,\n        n_ctx=0,  # 0 means that the model limit will be used, instead of the default (512) or other hardcoded value\n        verbose=False,\n        n_gpu_layers=-1 if torch.cuda.is_available() else 0,\n    )\n    return model\n</code></pre>"},{"location":"api/#document_to_podcast.inference.model_loaders.TTS_LOADERS","title":"<code>document_to_podcast.inference.model_loaders.TTS_LOADERS = {'OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf': _load_oute_tts, 'OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf': _load_oute_tts, 'hexgrad/kLegacy/v0.19/kokoro-v0_19.pth': _load_kokoro_legacy_tts}</code>  <code>module-attribute</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_text","title":"<code>document_to_podcast.inference.text_to_text</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_text.text_to_text","title":"<code>text_to_text(input_text, model, system_prompt, return_json=True, stop=None)</code>","text":"<p>Transforms input_text using the given model and system prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to be transformed.</p> required <code>model</code> <code>Llama</code> <p>The model to use for conversion.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt to use for conversion.</p> required <code>return_json</code> <code>bool</code> <p>Whether to return the response as JSON. Defaults to True.</p> <code>True</code> <code>stop</code> <code>str | list[str] | None</code> <p>The stop token(s).</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The full transformed text.</p> Source code in <code>src/document_to_podcast/inference/text_to_text.py</code> <pre><code>def text_to_text(\n    input_text: str,\n    model: Llama,\n    system_prompt: str,\n    return_json: bool = True,\n    stop: str | list[str] | None = None,\n) -&gt; str:\n    \"\"\"\n    Transforms input_text using the given model and system prompt.\n\n    Args:\n        input_text (str): The text to be transformed.\n        model (Llama): The model to use for conversion.\n        system_prompt (str): The system prompt to use for conversion.\n        return_json (bool, optional): Whether to return the response as JSON.\n            Defaults to True.\n        stop (str | list[str] | None, optional): The stop token(s).\n\n    Returns:\n        str: The full transformed text.\n    \"\"\"\n    response = chat_completion(\n        input_text, model, system_prompt, return_json, stop=stop, stream=False\n    )\n    return response[\"choices\"][0][\"message\"][\"content\"]\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_text.text_to_text_stream","title":"<code>text_to_text_stream(input_text, model, system_prompt, return_json=True, stop=None)</code>","text":"<p>Transforms input_text using the given model and system prompt.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to be transformed.</p> required <code>model</code> <code>Llama</code> <p>The model to use for conversion.</p> required <code>system_prompt</code> <code>str</code> <p>The system prompt to use for conversion.</p> required <code>return_json</code> <code>bool</code> <p>Whether to return the response as JSON. Defaults to True.</p> <code>True</code> <code>stop</code> <code>str | list[str] | None</code> <p>The stop token(s).</p> <code>None</code> <p>Yields:</p> Name Type Description <code>str</code> <code>str</code> <p>Chunks of the transformed text as they are available.</p> Source code in <code>src/document_to_podcast/inference/text_to_text.py</code> <pre><code>def text_to_text_stream(\n    input_text: str,\n    model: Llama,\n    system_prompt: str,\n    return_json: bool = True,\n    stop: str | list[str] | None = None,\n) -&gt; Iterator[str]:\n    \"\"\"\n    Transforms input_text using the given model and system prompt.\n\n    Args:\n        input_text (str): The text to be transformed.\n        model (Llama): The model to use for conversion.\n        system_prompt (str): The system prompt to use for conversion.\n        return_json (bool, optional): Whether to return the response as JSON.\n            Defaults to True.\n        stop (str | list[str] | None, optional): The stop token(s).\n\n    Yields:\n        str: Chunks of the transformed text as they are available.\n    \"\"\"\n    response = chat_completion(\n        input_text, model, system_prompt, return_json, stop=stop, stream=True\n    )\n    for item in response:\n        if item[\"choices\"][0].get(\"delta\", {}).get(\"content\", None):\n            yield item[\"choices\"][0].get(\"delta\", {}).get(\"content\", None)\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_speech","title":"<code>document_to_podcast.inference.text_to_speech</code>","text":""},{"location":"api/#document_to_podcast.inference.text_to_speech._text_to_speech_oute","title":"<code>_text_to_speech_oute(input_text, model, voice_profile, **kwargs)</code>","text":"<p>TTS generation function for the Oute TTS model family. Args:     input_text (str): The text to convert to speech.     model: A model from the Oute TTS family.     voice_profile: a pre-defined ID for the Oute models (e.g. \"female_1\")         more info here https://github.com/edwko/OuteTTS/tree/main/outetts/version/v1/default_speakers     temperature (float, default = 0.3): Controls the randomness of predictions by scaling the logits.         Lower values make the output more focused and deterministic, higher values produce more diverse results.     repetition_penalty (float, default = 1.1): Applies a penalty to tokens that have already been generated,         reducing the likelihood of repetition and enhancing text variety.     max_length (int, default = 4096): Defines the maximum number of tokens for the generated text sequence.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>numpy array: The waveform of the speech as a 2D numpy array</p> Source code in <code>src/document_to_podcast/inference/text_to_speech.py</code> <pre><code>def _text_to_speech_oute(\n    input_text: str,\n    model: InterfaceGGUF,\n    voice_profile: str,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"\n    TTS generation function for the Oute TTS model family.\n    Args:\n        input_text (str): The text to convert to speech.\n        model: A model from the Oute TTS family.\n        voice_profile: a pre-defined ID for the Oute models (e.g. \"female_1\")\n            more info here https://github.com/edwko/OuteTTS/tree/main/outetts/version/v1/default_speakers\n        temperature (float, default = 0.3): Controls the randomness of predictions by scaling the logits.\n            Lower values make the output more focused and deterministic, higher values produce more diverse results.\n        repetition_penalty (float, default = 1.1): Applies a penalty to tokens that have already been generated,\n            reducing the likelihood of repetition and enhancing text variety.\n        max_length (int, default = 4096): Defines the maximum number of tokens for the generated text sequence.\n\n    Returns:\n        numpy array: The waveform of the speech as a 2D numpy array\n    \"\"\"\n    speaker = model.load_default_speaker(name=voice_profile)\n\n    output = model.generate(\n        text=input_text,\n        temperature=kwargs.pop(\"temperature\", 0.3),\n        repetition_penalty=kwargs.pop(\"repetition_penalty\", 1.1),\n        max_length=kwargs.pop(\"max_length\", 4096),\n        speaker=speaker,\n    )\n\n    output_as_np = output.audio.cpu().detach().numpy().squeeze()\n    return output_as_np\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_speech.text_to_speech","title":"<code>text_to_speech(input_text, model, voice_profile)</code>","text":"<p>Generate speech from text using a TTS model.</p> <p>Parameters:</p> Name Type Description Default <code>input_text</code> <code>str</code> <p>The text to convert to speech.</p> required <code>model</code> <code>TTSModel</code> <p>The TTS model to use.</p> required <code>voice_profile</code> <code>str</code> <p>The voice profile to use for the speech. The format depends on the TTSModel used.</p> <p>For OuteTTS (the default), it should be a pre-defined ID like <code>female_1</code>. You can find all the IDs at this link</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The waveform of the speech as a 2D numpy array</p> Source code in <code>src/document_to_podcast/inference/text_to_speech.py</code> <pre><code>def text_to_speech(input_text: str, model: TTSModel, voice_profile: str) -&gt; np.ndarray:\n    \"\"\"\n    Generate speech from text using a TTS model.\n\n    Args:\n        input_text (str): The text to convert to speech.\n        model (TTSModel): The TTS model to use.\n        voice_profile (str): The voice profile to use for the speech.\n            The format depends on the TTSModel used.\n\n            For OuteTTS (the default), it should be a pre-defined ID like `female_1`.\n            You can find all the IDs [at this link](https://github.com/edwko/OuteTTS/tree/main/outetts/version/v1/default_speakers)\n\n    Returns:\n        np.ndarray: The waveform of the speech as a 2D numpy array\n    \"\"\"\n    return TTS_INFERENCE[model.model_id](\n        input_text, model.model, voice_profile, **model.custom_args\n    )\n</code></pre>"},{"location":"api/#document_to_podcast.inference.text_to_speech.TTS_INFERENCE","title":"<code>document_to_podcast.inference.text_to_speech.TTS_INFERENCE = {'OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf': _text_to_speech_oute, 'OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf': _text_to_speech_oute, 'hexgrad/kLegacy/v0.19/kokoro-v0_19.pth': _text_to_speech_kokoro}</code>  <code>module-attribute</code>","text":""},{"location":"cli/","title":"Command Line Interface","text":"<p>Once you have installed the blueprint, you can use it from the CLI.</p> <p>You can either provide the path to a configuration file:</p> <pre><code>document-to-podcast --from_config \"example_data/config.yaml\"\n</code></pre> <p>Or provide values to the arguments directly:</p> <pre><code>document-to-podcast \\\n--input_file \"example_data/Mozilla-Trustworthy_AI.pdf\" \\\n--output_folder \"example_data\"\n--text_to_text_model \"Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf\"\n</code></pre> <p>Note that you can also exit the podcast generation prematurely (before the whole podcast is created), by pressing Ctrl+C in the terminal. This will make the application stop the generation, but still save the result (script &amp; audio) to disk up until that point.</p>"},{"location":"cli/#document_to_podcast.cli.document_to_podcast","title":"<code>document_to_podcast.cli.document_to_podcast(input_file=None, output_folder=None, text_to_text_model='bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf', text_to_text_prompt=DEFAULT_PROMPT, text_to_speech_model='OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf', speakers=None, outetts_language='en', from_config=None)</code>","text":"<p>Generate a podcast from a document.</p> <p>Parameters:</p> Name Type Description Default <code>input_file</code> <code>str</code> <p>The path to the input file. Supported extensions:</p> <pre><code>- .pdf\n- .html\n- .txt\n- .docx\n- .md\n</code></pre> <code>None</code> <code>output_folder</code> <code>str</code> <p>The path to the output folder. Two files will be created:</p> <pre><code>- {output_folder}/podcast.txt\n- {output_folder}/podcast.wav\n</code></pre> <code>None</code> <code>text_to_text_model</code> <code>str</code> <p>The text-to-text model_id.</p> <p>Need to be formatted as <code>owner/repo/file</code>.</p> <p>Need to be a gguf file.</p> <p>Defaults to <code>bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf</code>.</p> <code>'bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf'</code> <code>text_to_text_prompt</code> <code>str</code> <p>The prompt for the text-to-text model. Defaults to DEFAULT_PROMPT.</p> <code>DEFAULT_PROMPT</code> <code>text_to_speech_model</code> <code>str</code> <p>The text-to-speech model_id. Defaults to <code>OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf</code>.</p> <code>'OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf'</code> <code>speakers</code> <code>list[Speaker] | None</code> <p>The speakers for the podcast. Defaults to DEFAULT_SPEAKERS.</p> <code>None</code> <code>outetts_language</code> <code>str</code> <p>For OuteTTS models we need to specify which language to use. Supported languages in 0.2-500M: en, zh, ja, ko. More info: https://github.com/edwko/OuteTTS</p> <code>'en'</code> <code>from_config</code> <code>str</code> <p>The path to the config file. Defaults to None.</p> <p>If provided, all other arguments will be ignored.</p> <code>None</code> Source code in <code>src/document_to_podcast/cli.py</code> <pre><code>@logger.catch(reraise=True)\ndef document_to_podcast(\n    input_file: str | None = None,\n    output_folder: str | None = None,\n    text_to_text_model: str = \"bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf\",\n    text_to_text_prompt: str = DEFAULT_PROMPT,\n    text_to_speech_model: str = \"OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf\",\n    speakers: list[Speaker] | None = None,\n    outetts_language: str = \"en\",  # Only applicable to OuteTTS models\n    from_config: str | None = None,\n):\n    \"\"\"\n    Generate a podcast from a document.\n\n    Args:\n        input_file (str): The path to the input file.\n            Supported extensions:\n\n                - .pdf\n                - .html\n                - .txt\n                - .docx\n                - .md\n\n        output_folder (str): The path to the output folder.\n            Two files will be created:\n\n                - {output_folder}/podcast.txt\n                - {output_folder}/podcast.wav\n\n        text_to_text_model (str, optional): The text-to-text model_id.\n\n            Need to be formatted as `owner/repo/file`.\n\n            Need to be a gguf file.\n\n            Defaults to `bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf`.\n\n        text_to_text_prompt (str, optional): The prompt for the text-to-text model.\n            Defaults to DEFAULT_PROMPT.\n\n        text_to_speech_model (str, optional): The text-to-speech model_id.\n            Defaults to `OuteAI/OuteTTS-0.2-500M-GGUF/OuteTTS-0.2-500M-FP16.gguf`.\n\n        speakers (list[Speaker] | None, optional): The speakers for the podcast.\n            Defaults to DEFAULT_SPEAKERS.\n\n        outetts_language (str): For OuteTTS models we need to specify which language to use.\n            Supported languages in 0.2-500M: en, zh, ja, ko. More info: https://github.com/edwko/OuteTTS\n\n        from_config (str, optional): The path to the config file. Defaults to None.\n\n            If provided, all other arguments will be ignored.\n    \"\"\"\n    if from_config:\n        config = Config.model_validate(yaml.safe_load(Path(from_config).read_text()))\n    else:\n        speakers = speakers or DEFAULT_SPEAKERS\n        config = Config(\n            input_file=input_file,\n            output_folder=output_folder,\n            text_to_text_model=text_to_text_model,\n            text_to_text_prompt=text_to_text_prompt,\n            text_to_speech_model=text_to_speech_model,\n            speakers=[Speaker.model_validate(speaker) for speaker in speakers],\n            outetts_language=outetts_language,\n        )\n\n    output_folder = Path(config.output_folder)\n    output_folder.mkdir(parents=True, exist_ok=True)\n\n    data_loader = DATA_LOADERS[Path(config.input_file).suffix]\n    logger.info(f\"Loading {config.input_file}\")\n    raw_text = data_loader(config.input_file)\n    logger.debug(f\"Loaded {len(raw_text)} characters\")\n\n    data_cleaner = DATA_CLEANERS[Path(config.input_file).suffix]\n    logger.info(f\"Cleaning {config.input_file}\")\n    clean_text = data_cleaner(raw_text)\n    logger.debug(f\"Cleaned {len(raw_text) - len(clean_text)} characters\")\n    logger.debug(f\"Length of cleaned text: {len(clean_text)}\")\n\n    logger.info(f\"Loading {config.text_to_text_model}\")\n    text_model = load_llama_cpp_model(model_id=config.text_to_text_model)\n\n    logger.info(f\"Loading {config.text_to_speech_model}\")\n    speech_model = load_tts_model(\n        model_id=config.text_to_speech_model, outetts_language=outetts_language\n    )\n\n    # ~4 characters per token is considered a reasonable default.\n    max_characters = text_model.n_ctx() * 4\n    if len(clean_text) &gt; max_characters:\n        logger.warning(\n            f\"Input text is too big ({len(clean_text)}).\"\n            f\" Using only a subset of it ({max_characters}).\"\n        )\n    clean_text = clean_text[:max_characters]\n\n    logger.info(\"Generating Podcast...\")\n    podcast_script = \"\"\n    text = \"\"\n    podcast_audio = []\n    system_prompt = config.text_to_text_prompt.strip()\n    system_prompt = system_prompt.replace(\n        \"{SPEAKERS}\", \"\\n\".join(str(speaker) for speaker in config.speakers)\n    )\n    try:\n        for chunk in text_to_text_stream(\n            clean_text, text_model, system_prompt=system_prompt\n        ):\n            text += chunk\n            podcast_script += chunk\n            if text.endswith(\"\\n\") and \"Speaker\" in text:\n                logger.debug(text)\n                speaker_id = re.search(r\"Speaker (\\d+)\", text).group(1)\n                voice_profile = next(\n                    speaker.voice_profile\n                    for speaker in config.speakers\n                    if speaker.id == int(speaker_id)\n                )\n                speech = text_to_speech(\n                    text.split(f'\"Speaker {speaker_id}\":')[-1],\n                    speech_model,\n                    voice_profile,\n                )\n                podcast_audio.append(speech)\n                text = \"\"\n\n    except KeyboardInterrupt:\n        logger.warning(\"Podcast generation stopped by user.\")\n    logger.info(\"Saving Podcast...\")\n    complete_audio = stack_audio_segments(\n        podcast_audio, sample_rate=speech_model.sample_rate, silence_pad=1.0\n    )\n\n    sf.write(\n        str(output_folder / \"podcast.wav\"),\n        complete_audio,\n        samplerate=speech_model.sample_rate,\n    )\n    (output_folder / \"podcast.txt\").write_text(podcast_script)\n    logger.success(\"Done!\")\n</code></pre>"},{"location":"cli/#document_to_podcast.config.Config","title":"<code>document_to_podcast.config.Config</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/document_to_podcast/config.py</code> <pre><code>class Config(BaseModel):\n    input_file: Annotated[FilePath, AfterValidator(validate_input_file)]\n    output_folder: str\n    text_to_text_model: Annotated[str, AfterValidator(validate_text_to_text_model)]\n    text_to_text_prompt: Annotated[str, AfterValidator(validate_text_to_text_prompt)]\n    text_to_speech_model: Annotated[str, AfterValidator(validate_text_to_speech_model)]\n    speakers: list[Speaker]\n    outetts_language: str = \"en\"\n</code></pre>"},{"location":"cli/#document_to_podcast.config.Speaker","title":"<code>document_to_podcast.config.Speaker</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/document_to_podcast/config.py</code> <pre><code>class Speaker(BaseModel):\n    id: int\n    name: str\n    description: str\n    voice_profile: str\n\n    def __str__(self):\n        return f\"Speaker {self.id}. Named {self.name}. {self.description}\"\n</code></pre>"},{"location":"cli/#document_to_podcast.config.DEFAULT_PROMPT","title":"<code>document_to_podcast.config.DEFAULT_PROMPT = '\\nYou are a podcast scriptwriter generating engaging and natural-sounding conversations in JSON format.\\nThe script features the following speakers:\\n{SPEAKERS}\\nInstructions:\\n- Write dynamic, easy-to-follow dialogue.\\n- Include natural interruptions and interjections.\\n- Avoid repetitive phrasing between speakers.\\n- Format output as a JSON conversation.\\nExample:\\n{\\n  \"Speaker 1\": \"Welcome to our podcast! Today, we\\'re exploring...\",\\n  \"Speaker 2\": \"Hi! I\\'m excited to hear about this. Can you explain...\",\\n  \"Speaker 1\": \"Sure! Imagine it like this...\",\\n  \"Speaker 2\": \"Oh, that\\'s cool! But how does...\"\\n}\\n'</code>  <code>module-attribute</code>","text":""},{"location":"cli/#document_to_podcast.config.DEFAULT_SPEAKERS","title":"<code>document_to_podcast.config.DEFAULT_SPEAKERS = [{'id': 1, 'name': 'Laura', 'description': 'The main host. She explains topics clearly using anecdotes and analogies, teaching in an engaging and captivating way.', 'voice_profile': 'female_1'}, {'id': 2, 'name': 'Jon', 'description': 'The co-host. He keeps the conversation on track, asks curious follow-up questions, and reacts with excitement or confusion, often using interjections like hmm or umm.', 'voice_profile': 'male_1'}]</code>  <code>module-attribute</code>","text":""},{"location":"customization/","title":"\ud83c\udfa8 Customization Guide","text":"<p>The Document-to-Podcast Blueprint is designed to be flexible and adaptable to your specific needs. This guide outlines the key parameters you can customize and explains how to make these changes depending on whether you\u2019re running the application via app.py or the CLI pipeline.</p>"},{"location":"customization/#supported-models","title":"\ud83e\udde0 Supported models","text":"<p>There are 2 different parameters to customize the models being used:</p>"},{"location":"customization/#text_to_text_model","title":"<code>text_to_text_model</code>","text":"<p>The language model used to generate the podcast script.</p> <p>Any model that can be loaded by <code>LLama.from_pretrained</code> can be used here.</p> <p>Format is expected to be <code>{org}/{repo}/{filename}</code>. For example: <code>Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf</code>.</p>"},{"location":"customization/#text_to_speech_model","title":"<code>text_to_speech_model</code>","text":"<p>The model used to generate the audio from the podcast script.</p> <p>You can use any of the models listed in <code>TTS_LOADERS</code> out of the box. We currently support OuteTTS.</p> <p>If you want to use a different model, you can integrate it by implementing the <code>_load</code> and <code>_text_to_speech</code> functions and registering them in <code>TTS_LOADERS</code> and <code>TTS_INFERENCE</code>. You can check this repo where different text-to-speech models are integrated.</p>"},{"location":"customization/#other-customizable-parameters","title":"\ud83d\udd8b\ufe0f Other Customizable Parameters","text":"<ul> <li> <p><code>input_file</code>: The input file specifies the document to be processed. Supports the following formats: <code>pdf</code>, <code>html</code>, <code>txt</code>, <code>docx</code>, <code>md</code>.</p> </li> <li> <p><code>text_to_text_prompt</code>: Defines the tone, structure, and instructions for generating the podcast script. This prompt is crucial for tailoring the conversation style to your project.</p> </li> <li> <p><code>speakers</code>: Defines the podcast participants, including their names, roles, descriptions, and voice profiles. Customize this to create engaging personas and voices for your podcast.</p> </li> </ul>"},{"location":"customization/#customizing-when-running-via-the-cli","title":"\u2328\ufe0f Customizing When Running via the CLI","text":"<p>If you\u2019re running the pipeline from the command line, you can customize the parameters by modifying the <code>example_data/config.yaml</code> file.</p> <p>Running in the CLI: <pre><code>document-to-podcast --from_config example_data/config.yaml\n</code></pre></p>"},{"location":"customization/#steps-to-customize","title":"Steps to Customize","text":"<ol> <li>Open the <code>config.yaml</code> file.</li> <li>Locate the parameter you want to adjust.</li> <li>Update the value and save the file.</li> </ol>"},{"location":"customization/#example-changing-the-text-to-text-model","title":"Example: Changing the Text-to-Text Model","text":"<p>In <code>config.yaml</code>, modify the <code>text_to_text_model</code> entry:</p> <pre><code>text_to_text_model: \"Qwen/Qwen2.5-1.5B-Instruct-GGUF/qwen2.5-1.5b-instruct-q8_0.gguf\"\n</code></pre>"},{"location":"customization/#customizing-when-running-via-apppy","title":"\ud83d\udda5\ufe0f Customizing When Running via <code>app.py</code>","text":"<p>If you\u2019re running the application using <code>app.py</code>, you can customize these parameters in the <code>src/config.py</code> file. This centralized configuration file simplifies the customization process.</p> <p>Running app.py: <pre><code>python -m streamlit run demo/app.py\n</code></pre></p>"},{"location":"customization/#steps-to-customize_1","title":"Steps to Customize","text":"<ol> <li>Open the <code>config.py</code> file.</li> <li>Locate the relevant parameter you want to change (e.g., <code>text_to_text_model</code>, <code>speakers</code>).</li> <li>Update the value according to your needs.</li> </ol>"},{"location":"customization/#example-updating-the-prompt","title":"Example: Updating the Prompt","text":"<p>In <code>config.py</code>, modify the <code>text_to_text_prompt</code> parameter:</p> <pre><code>DEFAULT_PROMPT = \"\"\"\nYou are a podcast scriptwriter generating engaging and humorous conversations in JSON format.\nThe script features the following speakers:\n{SPEAKERS}\nInstructions:\n- Use a casual and fun tone.\n- Include jokes and lighthearted banter.\n- Format output as a JSON conversation.\n  {\n    \"Speaker 1\": \"Well we a have a hilarious podcast in store for you today...\",\n    \"Speaker 2\": \"I can't wait, I had the weirdest week - let me tell you all about it...\",\n\"\"\"\n</code></pre>"},{"location":"customization/#customization-examples","title":"\u270f\ufe0f Customization Examples","text":"<p>Looking for inspiration? Check out these examples of how others have customized the Document-to-Podcast Blueprint for their unique needs:</p> <ul> <li>Radio Drama Generator: A creative adaptation that generates radio dramas by customizing ng the Blueprint parameters.</li> <li>Readme-to-Podcast: This project transforms GitHub README files into podcast-style audio, showcasing the Blueprint\u2019s ability to handle diverse text inputs.</li> <li>Multilingual Podcast: A repo that showcases how to use this package in other languages, like Hindi, Polish, Korean and many more.</li> </ul>"},{"location":"customization/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"},{"location":"future-features-contributions/","title":"\ud83d\ude80 Future Features &amp; Contributions","text":"<p>The Document-to-Podcast Blueprint is an evolving project designed to grow with the help of the open-source community. Whether you\u2019re an experienced developer or just starting, there are many ways you can contribute and help shape the future of this tool.</p>"},{"location":"future-features-contributions/#how-you-can-contribute","title":"\ud83c\udf1f How You Can Contribute","text":""},{"location":"future-features-contributions/#enhance-the-blueprint","title":"\ud83d\udee0\ufe0f Enhance the Blueprint","text":"<ul> <li>Check the Issues page to see if there are feature requests you'd like to implement</li> <li>Refer to our Contribution Guide for more details on contributions</li> </ul>"},{"location":"future-features-contributions/#extensibility-ideas","title":"\ud83c\udfa8 Extensibility Ideas","text":"<p>This Blueprint is designed to be a foundation you can build upon. By extending its capabilities, you can open the door to new applications, improve user experience, and adapt the Blueprint to address other use cases. Here are a few ideas for how you can expand its potential:</p> <ul> <li>New modalities input: Add support to the Blueprint to be able to handle different input modalities, like audio or images, enabling more flexibility in podcast generation.</li> <li>Improved audio quality: Explore and integrate more advanced open-source TTS frameworks to enhance the quality of generated audio, making podcasts sound more natural.</li> </ul> <p>We\u2019d love to see how you can enhance this Blueprint! If you create improvements or extend its capabilities, consider contributing them back to the project so others in the community can benefit from your work. Check out our Contributions Guide to get started!</p>"},{"location":"future-features-contributions/#share-your-ideas","title":"\ud83d\udca1 Share Your Ideas","text":"<p>Got an idea for how this Blueprint could be improved? You can share your suggestions through GitHub Issues.</p>"},{"location":"future-features-contributions/#build-new-blueprints","title":"\ud83c\udf0d Build New Blueprints","text":"<p>This project is part of a larger initiative to create a collection of reusable starter code solutions that use open-source AI tools. If you\u2019re inspired to create your own Blueprint, you can use the Blueprint-template to get started.</p> <p>Your contributions help make this Blueprint better for everyone \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#get-started-with-document-to-podcast-using-one-of-the-options-below","title":"Get started with Document-to-Podcast using one of the options below:","text":""},{"location":"getting-started/#setup-options","title":"Setup options","text":"\u2601\ufe0f Google Colab (GPU)\u2601\ufe0f GitHub Codespaces\ud83d\udcbb Local Installation <p>The easiest way to play with the code on a GPU, for free.</p> <p>Click the button below to launch the project directly in Google Colab:</p> <p><p></p></p> <p>Click the button below to launch the project directly in GitHub Codespaces:</p> <p><p></p></p> <p>Once the Codespaces environment launches, inside the terminal, start the Streamlit demo by running:</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre> <p>You can install the project from Pypi:</p> <pre><code>pip install document-to-podcast\n</code></pre> <p>Check the Command Line Interface guide.</p> <p>Alternatively, you can clone and install it in editable mode:</p> <ol> <li> <p>Clone the Repository</p> <pre><code>git clone https://github.com/mozilla-ai/document-to-podcast.git\ncd document-to-podcast\n</code></pre> </li> <li> <p>Install the project and its Dependencies</p> <pre><code>pip install -e .\n</code></pre> </li> <li> <p>Run the Demo</p> <pre><code>python -m streamlit run demo/app.py\n</code></pre> </li> </ol>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>When starting up the codespace, I get the message <code>Oh no, it looks like you are offline!</code></p> <p>If you are on Firefox and have Enhanced Tracking Protection <code>On</code>, try turning it <code>Off</code> for the codespace webpage.</p> <p>During the installation of the package, it fails with <code>ERROR: Failed building wheel for llama-cpp-python</code></p> <p>You are probably missing the <code>GNU Make</code> package. A quick way to solve it is run on your terminal <code>sudo apt install build-essential</code></p>"},{"location":"step-by-step-guide/","title":"Step-by-Step Guide: How the Document-to-Podcast Blueprint Works","text":"<p>Transforming static documents into engaging podcast episodes involves an integration of pre-processing, LLM-powered transcript generation, and text-to-speech generation. Here's how it all works under the hood:</p>"},{"location":"step-by-step-guide/#overview","title":"Overview","text":"<p>This system has three core stages:</p> <p>\ud83d\udcc4 1. Document Pre-Processing    Prepare the input document by extracting and cleaning the text.</p> <p>\ud83d\udcdc 2. Podcast Script Generation    Use an LLM to transform the cleaned text into a conversational podcast script.</p> <p>\ud83c\udf99\ufe0f 3. Audio Podcast Generation    Convert the script into an engaging audio podcast with distinct speaker voices.</p> <p>We'll also look at how <code>app.py</code> brings all these steps together to build an end-to-end demo application.</p> <p>First, let\u2019s dive into each step to understand how this works in practice.</p>"},{"location":"step-by-step-guide/#step-1-document-pre-processing","title":"Step 1: Document Pre-Processing","text":"<p>The process begins with preparing the input document for AI processing. The system handles various document types while ensuring the extracted content is clean and structured.</p> <p>Cleaner input data ensures that the model works with reliable and consistent information, reducing the likelihood of confusing with unexpected tokens and therefore helping it to generate better outputs.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - File Loading</p> <ul> <li> <p>Uses functions defined in <code>data_loaders.py</code></p> </li> <li> <p>Supports <code>.html</code>, <code>.pdf</code>, <code>.txt</code>, and <code>.docx</code> formats.</p> </li> <li> <p>Extracts readable text from uploaded files using specialized loaders.</p> </li> </ul> <p>2 - Text Cleaning</p> <ul> <li> <p>Uses functions defined in <code>data_cleaners.py</code></p> </li> <li> <p>Removes unwanted elements like URLs, email addresses, and special characters using Python's <code>re</code> library, which leverages Regular Expressions (regex) to identify and manipulate specific patterns in text.</p> </li> <li> <p>Ensures the document is clean and ready for the next step.</p> </li> </ul>"},{"location":"step-by-step-guide/#api-example","title":"\ud83d\udd0d API Example","text":"<pre><code>from document_to_podcast.preprocessing import DATA_CLEANERS, DATA_LOADERS\n\ninput_file = \"example_data/introducing-mozilla-ai-investing-in-trustworthy-ai.html\"\ndata_loader = DATA_LOADERS[\".html\"]\ndata_cleaner = DATA_CLEANERS[\".html\"]\n\nraw_data = data_loader(input_file)\nprint(raw_data[:200])\n\"\"\"\n&lt;!doctype html&gt;\n&lt;html class=\"no-js\" lang=\"en-US\"&gt;\n\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\"&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n  &lt;link rel=\"profile\" href=\"https://gmpg.org/x\n\"\"\"\nclean_data = data_cleaner(raw_data)\nprint(clean_data[:200])\n\"\"\"\nSkip to content Mozilla Internet Culture Deep Dives Mozilla Explains Interviews Videos Privacy Security Products Firefox Pocket Mozilla VPN Mozilla News Internet Policy Leadership Mitchell Baker, CEO\n\"\"\"\n</code></pre>"},{"location":"step-by-step-guide/#step-2-podcast-script-generation","title":"Step 2: Podcast Script Generation","text":"<p>In this step, the pre-processed text is transformed into a conversational podcast transcript. Using a Language Model, the system generates a dialogue that\u2019s both informative and engaging.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step_1","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - Model Loading</p> <ul> <li> <p>The <code>model_loader.py</code> module is responsible for loading the <code>text-to-text</code> models using the <code>llama_cpp</code> library.</p> </li> <li> <p>The function <code>load_llama_cpp_model</code> takes a model ID in the format <code>{org}/{repo}/{filename}</code> and loads the specified model. This approach of using the <code>llama_cpp</code> library supports efficient CPU-based inference, making language models accessible even on machines without GPUs.</p> </li> </ul> <p>2 - Text-to-Text Generation</p> <ul> <li> <p>The <code>text_to_text.py</code> script manages the interaction with the language model, converting input text into a structured conversational podcast script.</p> </li> <li> <p>It uses the <code>chat_completion</code> function to process the input text and a customizable system prompt, guiding the language to generate a text output (e.g. a coherent podcast script between speakers).</p> </li> <li> <p>The <code>return_json</code> parameter allows the output to be formatted as a JSON object style, which can make it easier to parse and integrate structured responses into applications.</p> </li> <li> <p>Supports both single-pass outputs (<code>text_to_text</code>) and real-time streamed responses (<code>text_to_text_stream</code>), offering flexibility for different use cases.</p> </li> </ul>"},{"location":"step-by-step-guide/#api-example_1","title":"\ud83d\udd0d API Example","text":"<pre><code>from document_to_podcast.inference.model_loaders import load_llama_cpp_model\nfrom document_to_podcast.inference.text_to_text import text_to_text, text_to_text_stream\n\n# Load the model\nmodel = load_llama_cpp_model(\n    \"bartowski/Qwen2.5-7B-Instruct-GGUF/Qwen2.5-7B-Instruct-Q8_0.gguf\"\n)\n\n# Define your input and system prompt\ninput_text = (\n    \"Electric vehicles (EVs) have seen a significant rise in adoption over the past \"\n    \"decade, driven by advancements in battery technology, government incentives, \"\n    \"and growing consumer awareness of environmental issues.\"\n)\n\nsystem_prompt = (\n    \"\"\"\n    You are a podcast scriptwriter generating engaging and natural-sounding conversations in JSON format.\n    - Write dynamic, easy-to-follow dialogue.\n    - Include natural interruptions and interjections.\n    - Avoid repetitive phrasing between speakers.\n    - Format output as a JSON conversation.\n    Example:\n    {\n      \"Speaker 1\": \"Welcome to our podcast! Today, we're exploring...\",\n      \"Speaker 2\": \"Hi! I'm excited to hear about this. Can you explain...\",\n    }\n    \"\"\"\n)\n\n# Generate a podcast script from the input text\npodcast_script = text_to_text(input_text, model, system_prompt)\nprint(podcast_script)\n\n\"\"\"\n{\n  \"Speaker 1\": \"Welcome to our podcast! Today, we're exploring the rise of electric vehicles (EVs) and what's driving this significant increase in adoption over the past decade.\",\n  \"Speaker 2\": \"Absolutely, it's fascinating to see how the market has evolved and how consumers are becoming more environmentally conscious.\",\n  \"Speaker 1\": \"Absolutely! Let's dive into the key factors driving this growth.\",\n  \"Speaker 2\": \"Sure, here are a few key drivers: advancements in battery technology, government incentives, and growing consumer awareness of environmental issues.\",\n  ...\n}\n\"\"\"\n\n# Example of real-time script generation with streaming\nfor chunk in text_to_text_stream(input_text, model, system_prompt):\n    print(chunk, end=\"\")\n</code></pre>"},{"location":"step-by-step-guide/#step-3-audio-podcast-generation","title":"Step 3: Audio Podcast Generation","text":"<p>In this final step, the generated podcast transcript is brought to life as an audio file. Using a Text-to-Speech (TTS) model, each speaker in the script is assigned a unique voice, creating an engaging and professional-sounding podcast.</p>"},{"location":"step-by-step-guide/#key-components-in-this-step_2","title":"\u2699\ufe0f Key Components in this Step","text":"<p>1 - Model Loading</p> <ul> <li> <p>The <code>model_loader.py</code> module is responsible for loading the <code>text-to-text</code> and <code>text-to-speech</code> models.</p> </li> <li> <p>The function <code>load_outetts_model</code> takes a model ID in the format <code>{org}/{repo}/{filename}</code> and loads the specified model, either on CPU or GPU, based on the <code>device</code> parameter. The parameter <code>language</code> also enables to swap between the languages the Oute package supports (as of Dec 2024: <code>en, zh, ja, ko</code>)</p> </li> </ul> <p>2 - Text-to-Speech Audio Generation</p> <ul> <li> <p>The <code>text_to_speech.py</code> script converts text into audio using a specified TTS model.</p> </li> <li> <p>A speaker profile defines the voice characteristics (e.g., tone, speed, clarity) for each speaker. This is specific to each TTS package. Oute models require one of the IDs specified here.</p> </li> <li> <p>The function <code>text_to_speech</code> takes the input text (e.g. podcast script) and speaker profile, generating a waveform (audio data in a numpy array) that represents the spoken version of the text.</p> </li> </ul>"},{"location":"step-by-step-guide/#api-example_2","title":"\ud83d\udd0d API Example","text":"<pre><code>import soundfile as sf\nfrom document_to_podcast.inference.model_loaders import load_outetts_model\nfrom document_to_podcast.inference.text_to_speech import text_to_speech\n\n# Load the TTS model\nmodel = load_outetts_model(\n    \"OuteAI/OuteTTS-0.1-350M-GGUF/OuteTTS-0.1-350M-FP16.gguf\"\n)\n\n# Generate the waveform\nwaveform = text_to_speech(\n    input_text=\"Welcome to our amazing podcast\",\n    model=model,\n    voice_profile=\"male_1\"\n)\n\n# Save the audio file\nsf.write(\n    \"podcast.wav\",\n    waveform,\n    samplerate=model.sample_rate\n)\n</code></pre>"},{"location":"step-by-step-guide/#bringing-it-all-together-in-apppy","title":"Bringing It All Together in <code>app.py</code>","text":"<p>The <code>app.py</code> demo app is shows you how all the components of the Document-to-Podcast Blueprint can come together. It demonstrates how you can take the individual steps\u2014Document Pre-Processing, Podcast Script Generation, and Audio Podcast Generation\u2014and integrate them into a functional application. This is the heart of the Blueprint in action, showing how you can build an app using the provided tools and components.</p> <p>This demo uses Streamlit, an open-source Python framework for interactive apps.</p>"},{"location":"step-by-step-guide/#how-apppy-applies-each-step","title":"\ud83e\udde0 How <code>app.py</code> Applies Each Step","text":"<p>\ud83d\udcc4 Document Upload &amp; Pre-Processing</p> <ul> <li> <p>Users upload a file via the Streamlit interface (<code>st.file_uploader</code>), which supports <code>.pdf</code>, <code>.txt</code>, <code>.docx</code>, <code>.html</code>, and <code>.md</code> formats.</p> </li> <li> <p>The uploaded file is passed to the File Loading and Text Cleaning modules.</p> </li> <li> <p>Raw text is extracted using <code>DATA_LOADERS</code>, and the cleaned version is displayed alongside it using <code>DATA_CLEANERS</code>, and displayed to the end user.</p> </li> </ul> <p>\u2699\ufe0f Loading Models</p> <ul> <li> <p>The script uses <code>load_llama_cpp_model</code> from <code>model_loader.py</code> to load the LLM for generating the podcast script.</p> </li> <li> <p>Similarly, <code>load_outetts_model</code> is used to prepare the TTS model and tokenizer for audio generation.</p> </li> <li> <p>These models are cached using <code>@st.cache_resource</code> to ensure fast and efficient reuse during app interactions.</p> </li> </ul> <p>\ud83d\udcdd Podcast Script Generation</p> <ul> <li> <p>The cleaned text and a system-defined podcast prompt are fed into the text_to_text_stream function.</p> </li> <li> <p>The <code>DEFAULT_PROMPT</code> is loaded from <code>config.py</code></p> </li> <li> <p>The script is streamed back to the user in real-time, allowing them to see the generated conversation between speakers</p> </li> </ul> <p>\ud83c\udf99\ufe0f Podcast Generation</p> <ul> <li> <p>For each speaker in the podcast script, audio is generated using the <code>text_to_speech</code> function with distinct speaker profiles</p> </li> <li> <p>The <code>DEFAULT_SPEAKERS</code> is loaded from <code>config.py</code></p> </li> <li> <p>The generated audio is displayed with a player so users can listen directly in the app.</p> </li> </ul>"},{"location":"step-by-step-guide/#customizing-the-blueprint","title":"\ud83c\udfa8 Customizing the Blueprint","text":"<p>To better understand how you can tailor this Blueprint to suit your specific needs, please visit the Customization Guide.</p>"},{"location":"step-by-step-guide/#contributing-to-the-blueprint","title":"\ud83e\udd1d Contributing to the Blueprint","text":"<p>Want to help improve or extend this Blueprint? Check out the Future Features &amp; Contributions Guide to see how you can contribute your ideas, code, or feedback to make this Blueprint even better!</p>"}]}